{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten,Dropout\n",
    "from keras.layers import Conv2D,LSTM, BatchNormalization,MaxPooling2D,Reshape, GRU\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "\n",
    "    X_test = np.load(\"data/X_test.npy\")\n",
    "    y_test = np.load(\"data/y_test.npy\")\n",
    "    person_train_valid = np.load(\"data/person_train_valid.npy\")\n",
    "    person_train_valid = person_train_valid.reshape(2115)\n",
    "    X_train_valid = np.load(\"data/X_train_valid.npy\")\n",
    "    y_train_valid = np.load(\"data/y_train_valid.npy\")\n",
    "    person_test = np.load(\"data/person_test.npy\")\n",
    "\n",
    "    return X_test,y_test,person_train_valid,X_train_valid,y_train_valid,person_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(X,y,p,sub_sample,average,noise):\n",
    "    \n",
    "    total_X = None\n",
    "    total_y = None\n",
    "    total_p = None\n",
    "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
    "    X = X[:,:,0:500]\n",
    "    print('Shape of X after trimming:',X.shape)\n",
    "    \n",
    "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "    \n",
    "    \n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    total_p = p\n",
    "\n",
    "    print('Shape of X after maxpooling:',total_X.shape)\n",
    "    \n",
    "    # Averaging + noise \n",
    "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
    "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
    "    \n",
    "    total_X = np.vstack((total_X, X_average))\n",
    "    total_y = np.hstack((total_y, y))\n",
    "    total_p = np.hstack((total_p, p))\n",
    "    print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
    "    \n",
    "    # Subsampling\n",
    "    \n",
    "    for i in range(sub_sample):\n",
    "        \n",
    "        X_subsample = X[:, :, i::sub_sample] + \\\n",
    "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "            \n",
    "        total_X = np.vstack((total_X, X_subsample))\n",
    "        total_y = np.hstack((total_y, y))\n",
    "        total_p = np.hstack((total_p, p))\n",
    "        \n",
    "    \n",
    "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
    "    return total_X,total_y, total_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocessing the dataset\n",
    "def preprocessing(X_test,y_test,person_train_valid,X_train_valid,y_train_valid,person_test):\n",
    "\n",
    "    y_train_valid -= 769\n",
    "    y_test -= 769\n",
    "    \n",
    "    X_train_valid_prep,y_train_valid_prep,person_train_valid_prep = data_prep(X_train_valid,y_train_valid,person_train_valid,2,2,True)\n",
    "    X_test_prep,y_test_prep,person_test_prep = data_prep(X_test,y_test,person_test,2,2,True)\n",
    "    stratif_labels = []\n",
    "    \n",
    "    for i in range(person_train_valid_prep.shape[0]):\n",
    "        stratif_labels.append(str(person_train_valid_prep[i].astype('int'))+str(y_train_valid_prep[i]))\n",
    "        print(X_train_valid_prep.shape)\n",
    "    print(y_train_valid_prep.shape)\n",
    "    print(X_test_prep.shape)\n",
    "    print(y_test_prep.shape)\n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    total_size = y_train_valid_prep.shape[0]\n",
    "    num_samples = int(total_size*0.1773)\n",
    "    \n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(X_train_valid_prep,y_train_valid_prep,test_size=num_samples/total_size,stratify=stratif_labels)\n",
    "\n",
    "    print('Shape of training set:',x_train.shape)\n",
    "    print('Shape of validation set:',x_valid.shape)\n",
    "    print('Shape of training labels:',y_train.shape)\n",
    "    print('Shape of validation labels:',y_valid.shape)\n",
    "\n",
    "\n",
    "    # Converting the labels to categorical variables for multiclass classification\n",
    "    y_train = to_categorical(y_train, 4)\n",
    "    y_valid = to_categorical(y_valid, 4)\n",
    "    y_test = to_categorical(y_test_prep, 4)\n",
    "    print('Shape of training labels after categorical conversion:',y_train.shape)\n",
    "    print('Shape of validation labels after categorical conversion:',y_valid.shape)\n",
    "    print('Shape of test labels after categorical conversion:',y_test.shape)\n",
    "\n",
    "    # Adding width of the segment to be 1\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "    print('Shape of training set after adding width info:',x_train.shape)\n",
    "    print('Shape of validation set after adding width info:',x_valid.shape)\n",
    "    print('Shape of test set after adding width info:',x_test.shape)\n",
    "\n",
    "\n",
    "    # Reshaping the training and validation dataset\n",
    "    x_train = np.swapaxes(x_train, 1,3)\n",
    "    x_train = np.swapaxes(x_train, 1,2)\n",
    "    x_valid = np.swapaxes(x_valid, 1,3)\n",
    "    x_valid = np.swapaxes(x_valid, 1,2)\n",
    "    x_test = np.swapaxes(x_test, 1,3)\n",
    "    x_test = np.swapaxes(x_test, 1,2)\n",
    "    print('Shape of training set after dimension reshaping:',x_train.shape)\n",
    "    print('Shape of validation set after dimension reshaping:',x_valid.shape)\n",
    "    print('Shape of test set after dimension reshaping:',x_test.shape)\n",
    "    return y_train, y_valid, y_test, x_train, x_valid, x_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimal Preprocessing for Baseline Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_basic(X_test,y_test,person_train_valid,X_train_valid,y_train_valid,person_test):\n",
    "\n",
    "    y_train_valid -= 769\n",
    "    y_test -= 769\n",
    "    \n",
    "\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(X_train_valid_prep,y_train_valid_prep)\n",
    "\n",
    "\n",
    "    # Converting the labels to categorical variables for multiclass classification\n",
    "    y_train = to_categorical(y_train, 4)\n",
    "    y_valid = to_categorical(y_valid, 4)\n",
    "    y_test = to_categorical(y_test_prep, 4)\n",
    "\n",
    "    # Adding width of the segment to be 1\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "\n",
    "\n",
    "    # Reshaping the training and validation dataset\n",
    "    x_train = np.swapaxes(x_train, 1,3)\n",
    "    x_train = np.swapaxes(x_train, 1,2)\n",
    "    x_valid = np.swapaxes(x_valid, 1,3)\n",
    "    x_valid = np.swapaxes(x_valid, 1,2)\n",
    "    x_test = np.swapaxes(x_test, 1,3)\n",
    "    x_test = np.swapaxes(x_test, 1,2)\n",
    "    \n",
    "    return y_train, y_valid, y_test, x_train, x_valid, x_test\n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_by_subjects(subject_ids=[]):\n",
    "\n",
    "    # If no subject_ids are provided, the model will be trained on all subjects\n",
    "    if subject_ids == []:\n",
    "        X_test,y_test,person_train_valid,X_train_valid,y_train_valid,person_test = load_data()\n",
    "        y_train, y_valid, y_test, x_train, x_valid, x_test = preprocessing(X_test,y_test,person_train_valid,X_train_valid,y_train_valid,person_test)\n",
    "        cnn_lstm_hybrid_model = generate_cnn_lstm_hybrid_model()\n",
    "        return get_model_results(cnn_lstm_hybrid_model, x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    "        \n",
    "    test_accuracies = {}\n",
    "    \n",
    "    for i in subject_ids:\n",
    "        \n",
    "        X_test,y_test,person_train_valid,X_train_valid,y_train_valid,person_test = load_data()\n",
    "\n",
    "        X_train_valid = X_train_valid[np.where(person_train_valid == i)[0]]\n",
    "        y_train_valid = y_train_valid[np.where(person_train_valid == i)[0]]\n",
    "        X_test = X_test[np.where(person_test == i)[0]]\n",
    "        y_test = y_test[np.where(person_test == i)[0]]\n",
    "        person_train_valid = person_train_valid[np.where(person_train_valid == i)[0]]\n",
    "        person_test = person_test[np.where(person_test == i)[0]]\n",
    "        \n",
    "        y_train, y_valid, y_test, x_train, x_valid, x_test = preprocessing_basic(X_test,y_test,person_train_valid,X_train_valid,y_train_valid,person_test)\n",
    "\n",
    "        cnn_model = generate_basic_cnn_model()\n",
    "        test_accuracies[i] = get_model_results(cnn_model, x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    "    \n",
    "    return test_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_by_subjects([0,1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the CNN model using sequential class\n",
    "def generate_basic_cnn_model():\n",
    "    basic_cnn_model = Sequential()\n",
    "\n",
    "    # Conv. block 1\n",
    "    basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "    basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
    "    basic_cnn_model.add(BatchNormalization())\n",
    "    basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 2\n",
    "    basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    basic_cnn_model.add(BatchNormalization())\n",
    "    basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 3\n",
    "    basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    basic_cnn_model.add(BatchNormalization())\n",
    "    basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 4\n",
    "    basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    basic_cnn_model.add(BatchNormalization())\n",
    "    basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer with Softmax activation\n",
    "    basic_cnn_model.add(Flatten()) # Flattens the input\n",
    "    basic_cnn_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "    return basic_cnn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cnn_lstm_hybrid_model():\n",
    "    hybrid_cnn_lstm_model = Sequential()\n",
    "\n",
    "    # Conv. block 1\n",
    "    hybrid_cnn_lstm_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "    hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
    "    hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "    hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 2\n",
    "    hybrid_cnn_lstm_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "    hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 3\n",
    "    hybrid_cnn_lstm_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "    hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "    # FC+LSTM layers\n",
    "    hybrid_cnn_lstm_model.add(Flatten()) # Adding a flattening operation to the output of CNN block\n",
    "    hybrid_cnn_lstm_model.add(Dense((100))) # FC layer with 100 units\n",
    "    hybrid_cnn_lstm_model.add(Reshape((100,1))) # Reshape my output of FC layer so that it's compatible\n",
    "    hybrid_cnn_lstm_model.add(LSTM(100, dropout=0.6, recurrent_dropout=0.1, input_shape=(100,1), return_sequences=True))\n",
    "\n",
    "    hybrid_cnn_lstm_model.add(LSTM(70, dropout=0.6, recurrent_dropout=0.1, return_sequences=False))\n",
    "    # Output layer with Softmax activation \n",
    "    hybrid_cnn_lstm_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "    return hybrid_cnn_lstm_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Calculate Accuracies by Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(model, x_train, y_train, x_valid, y_valid, x_test, y_test): \n",
    "    \n",
    "    # Model parameters\n",
    "    learning_rate = 2e-3\n",
    "    epochs = 50\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "         optimizer=optimizer,\n",
    "         metrics=['accuracy'])\n",
    "\n",
    "    # Training and validating the model\n",
    "    model_results = model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=200,\n",
    "             epochs=epochs,\n",
    "             validation_data=(x_valid, y_valid), verbose=False)\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return score[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
