{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Flatten,Dropout\n",
    "from keras.layers import Conv2D,LSTM, BatchNormalization,MaxPooling2D,Reshape\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    X_test = np.load(\"data/X_test.npy\")\n",
    "    y_test = np.load(\"data/y_test.npy\")\n",
    "    person_train_valid = np.load(\"data/person_train_valid.npy\")\n",
    "    X_train_valid = np.load(\"data/X_train_valid.npy\")\n",
    "    y_train_valid = np.load(\"data/y_train_valid.npy\")\n",
    "    person_test = np.load(\"data/person_test.npy\")\n",
    "\n",
    "    return X_test,y_test,person_train_valid,X_train_valid,y_train_valid,person_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_prep(X,y,sub_sample,average,noise):\n",
    "    \n",
    "    total_X = None\n",
    "    total_y = None\n",
    "    \n",
    "    X = X[:,:,0:500]\n",
    "    \n",
    "    # Maxpooling the data \n",
    "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
    "    \n",
    "    \n",
    "    total_X = X_max\n",
    "    total_y = y\n",
    "    \n",
    "    # Averaging + noise \n",
    "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
    "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
    "    \n",
    "    total_X = np.vstack((total_X, X_average))\n",
    "    total_y = np.hstack((total_y, y))\n",
    "    \n",
    "    # Subsampling\n",
    "    \n",
    "    for i in range(sub_sample):\n",
    "        \n",
    "        X_subsample = X[:, :, i::sub_sample] + \\\n",
    "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
    "            \n",
    "        total_X = np.vstack((total_X, X_subsample))\n",
    "        total_y = np.hstack((total_y, y))\n",
    "    \n",
    "    return total_X,total_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(y_train_valid, y_test, X_train_valid, X_test):\n",
    "    y_train_valid -= 769\n",
    "    y_test -= 769\n",
    "\n",
    "    X_train_valid_prep,y_train_valid_prep = data_prep(X_train_valid,y_train_valid,2,2,True)\n",
    "    X_test_prep,y_test_prep = data_prep(X_test,y_test,2,2,True)\n",
    "\n",
    "    ## Random splitting and reshaping the data\n",
    "    total_size = y_train_valid_prep.shape[0]\n",
    "    num_samples = int(total_size*0.1773)\n",
    "\n",
    "    # First generating the training and validation indices using random splitting\n",
    "    ind_valid = np.random.choice(total_size, num_samples, replace=False)\n",
    "    ind_train = np.array(list(set(range(total_size)).difference(set(ind_valid))))\n",
    "\n",
    "    # Creating the training and validation sets using the generated indices\n",
    "    (x_train, x_valid) = X_train_valid_prep[ind_train], X_train_valid_prep[ind_valid] \n",
    "    (y_train, y_valid) = y_train_valid_prep[ind_train], y_train_valid_prep[ind_valid]\n",
    "\n",
    "\n",
    "    # Converting the labels to categorical variables for multiclass classification\n",
    "    y_train = to_categorical(y_train, 4)\n",
    "    y_valid = to_categorical(y_valid, 4)\n",
    "    y_test = to_categorical(y_test_prep, 4)\n",
    "\n",
    "    # Adding width of the segment to be 1\n",
    "    x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], x_train.shape[2], 1)\n",
    "    x_valid = x_valid.reshape(x_valid.shape[0], x_valid.shape[1], x_train.shape[2], 1)\n",
    "    x_test = X_test_prep.reshape(X_test_prep.shape[0], X_test_prep.shape[1], X_test_prep.shape[2], 1)\n",
    "\n",
    "\n",
    "    # Reshaping the training and validation dataset\n",
    "    x_train = np.swapaxes(x_train, 1,3)\n",
    "    x_train = np.swapaxes(x_train, 1,2)\n",
    "    x_valid = np.swapaxes(x_valid, 1,3)\n",
    "    x_valid = np.swapaxes(x_valid, 1,2)\n",
    "    x_test = np.swapaxes(x_test, 1,3)\n",
    "    x_test = np.swapaxes(x_test, 1,2)\n",
    "    \n",
    "    return y_train, y_valid, y_test, x_train, x_valid, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_basic_cnn_model(): \n",
    "    basic_cnn_model = Sequential()\n",
    "\n",
    "    # Conv. block 1\n",
    "    basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "    basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
    "    basic_cnn_model.add(BatchNormalization())\n",
    "    basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 2\n",
    "    basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    basic_cnn_model.add(BatchNormalization())\n",
    "    basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 3\n",
    "    basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    basic_cnn_model.add(BatchNormalization())\n",
    "    basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 4\n",
    "    basic_cnn_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    basic_cnn_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    basic_cnn_model.add(BatchNormalization())\n",
    "    basic_cnn_model.add(Dropout(0.5))\n",
    "\n",
    "    # Output layer with Softmax activation\n",
    "    basic_cnn_model.add(Flatten()) # Flattens the input\n",
    "    basic_cnn_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "    return basic_cnn_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cnn_lstm_hybrid_model():\n",
    "    hybrid_cnn_lstm_model = Sequential()\n",
    "\n",
    "    # Conv. block 1\n",
    "    hybrid_cnn_lstm_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu', input_shape=(250,1,22)))\n",
    "    hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same')) # Read the keras documentation\n",
    "    hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "    hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 2\n",
    "    hybrid_cnn_lstm_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "    hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "    # Conv. block 3\n",
    "    hybrid_cnn_lstm_model.add(Conv2D(filters=100, kernel_size=(10,1), padding='same', activation='elu'))\n",
    "    hybrid_cnn_lstm_model.add(MaxPooling2D(pool_size=(3,1), padding='same'))\n",
    "    hybrid_cnn_lstm_model.add(BatchNormalization())\n",
    "    hybrid_cnn_lstm_model.add(Dropout(0.5))\n",
    "\n",
    "    # FC+LSTM layers\n",
    "    hybrid_cnn_lstm_model.add(Flatten()) # Adding a flattening operation to the output of CNN block\n",
    "    hybrid_cnn_lstm_model.add(Dense((100))) # FC layer with 100 units\n",
    "    hybrid_cnn_lstm_model.add(Reshape((100,1))) # Reshape my output of FC layer so that it's compatible\n",
    "    hybrid_cnn_lstm_model.add(LSTM(100, dropout=0.6, recurrent_dropout=0.1, input_shape=(100,1), return_sequences=True))\n",
    "\n",
    "    hybrid_cnn_lstm_model.add(LSTM(70, dropout=0.6, recurrent_dropout=0.1, return_sequences=False))\n",
    "    # Output layer with Softmax activation \n",
    "    hybrid_cnn_lstm_model.add(Dense(4, activation='softmax')) # Output FC layer with softmax activation\n",
    "\n",
    "    return hybrid_cnn_lstm_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(model, x_train, y_train, x_valid, y_valid, x_test, y_test): \n",
    "    \n",
    "    # Model parameters\n",
    "    learning_rate = 1e-3\n",
    "    epochs = 50\n",
    "    optimizer = keras.optimizers.Adam(learning_rate)\n",
    "    \n",
    "    # Compiling the model\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "         optimizer=optimizer,\n",
    "         metrics=['accuracy'])\n",
    "\n",
    "    # Training and validating the model\n",
    "    model_results = model.fit(x_train,\n",
    "             y_train,\n",
    "             batch_size=200,\n",
    "             epochs=epochs,\n",
    "             validation_data=(x_valid, y_valid), verbose=True)\n",
    "\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    return score[1]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Calculate Accuracies by Subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_by_subjects_random_sampling(subject_ids=[]):\n",
    "    X_test,y_test,person_train_valid,X_train_valid,y_train_valid,person_test = load_data()\n",
    "\n",
    "    if subject_ids == []:\n",
    "        y_train, y_valid, y_test, x_train, x_valid, x_test = preprocessing(y_train_valid, y_test, X_train_valid, X_test)\n",
    "        basic_cnn_model = generate_basic_cnn_model()  \n",
    "        return get_model_results(basic_cnn_model, x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    "\n",
    "    test_accuracies = {}\n",
    "    for i in subject_ids:\n",
    "        X_test,y_test,person_train_valid,X_train_valid,y_train_valid,person_test = load_data()\n",
    "\n",
    "        X_train_valid = X_train_valid[np.where(person_train_valid==i)[0],:,:]\n",
    "        y_train_valid = y_train_valid[np.where(person_train_valid==i)[0]]\n",
    "        X_test = X_test[np.where(person_test==i)[0],:,:]\n",
    "        y_test = y_test[np.where(person_test==i)[0]]\n",
    "\n",
    "        y_train, y_valid, y_test, x_train, x_valid, x_test = preprocessing(y_train_valid, y_test, X_train_valid, X_test)\n",
    "        basic_cnn_model = generate_basic_cnn_model()  \n",
    "        test_accuracies[i] = get_model_results(basic_cnn_model, x_train, y_train, x_valid, y_valid, x_test, y_test)\n",
    "    return test_accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_by_subjects_random_sampling([0, 1, 2, 3, 4, 5, 6, 7, 8])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
